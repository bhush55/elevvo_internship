{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6324d3ec-9a2d-4a69-9e02-207ea1beebbf",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d385b0a2-5f2a-4a50-b0af-77da4bd00403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d48a59cc-1f18-4c93-bf72-fe4656fe56a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Named Entity Recognition (NER) from News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e3cd07-9cc2-4eb2-a1bb-83ec22cc1331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6f4209a-1d63-401c-8ee1-fd097fbd8065",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nisha\\anaconda3\\envs\\mp_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoConfig\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06f7d24c-aebb-4b66-995f-d155c7296e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 14041, Valid: 3250, Test: 3453\n",
      "Entity tags: ['_', 'B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "def load_sentences(filepath):\n",
    "    final = []\n",
    "    sentences = []\n",
    "    with open(filepath, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in f.readlines():\n",
    "            if line.strip() == \"\" or line.startswith(\"-DOCSTART-\"):\n",
    "                if len(sentences) > 0:\n",
    "                    final.append(sentences)\n",
    "                    sentences = []\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                sentences.append((parts[0], parts[-1]))\n",
    "    return final\n",
    "\n",
    "# Update this to your folder where train.txt, valid.txt, test.txt exist\n",
    "base_path = r\"C:\\Users\\Nisha\\Desktop\\Internship_2\\Task_4\\\\\"\n",
    "\n",
    "train_samples = load_sentences(base_path + \"train.txt\")\n",
    "valid_samples = load_sentences(base_path + \"valid.txt\")\n",
    "test_samples  = load_sentences(base_path + \"test.txt\")\n",
    "\n",
    "samples = train_samples + test_samples\n",
    "\n",
    "schema = ['_'] + sorted({tag for sentence in samples for _, tag in sentence})\n",
    "tag2id = {tag: i for i, tag in enumerate(schema)}\n",
    "id2tag = {i: tag for tag, i in tag2id.items()}\n",
    "\n",
    "print(f\"Train: {len(train_samples)}, Valid: {len(valid_samples)}, Test: {len(test_samples)}\")\n",
    "print(f\"Entity tags: {schema}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3f1db7c-072e-40f7-8fe4-c3c4906774eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize_and_align_labels(sentences):\n",
    "    input_ids = []\n",
    "    labels = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        tokens = []\n",
    "        tag_ids = []\n",
    "\n",
    "        for word, tag in sentence:\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            if not word_tokens:\n",
    "                continue\n",
    "            tokens.extend(word_tokens)\n",
    "            tag_ids.extend([tag2id[tag]] + [tag2id['O']]*(len(word_tokens)-1))\n",
    "\n",
    "        tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "        tag_ids = [tag2id['O']] + tag_ids + [tag2id['O']]\n",
    "\n",
    "        input_ids.append(tokenizer.convert_tokens_to_ids(tokens))\n",
    "        labels.append(tag_ids)\n",
    "\n",
    "    max_len = max(len(x) for x in input_ids)\n",
    "    input_ids = [x + [0]*(max_len-len(x)) for x in input_ids]\n",
    "    labels = [x + [tag2id['O']]*(max_len-len(x)) for x in labels]\n",
    "\n",
    "    return torch.tensor(input_ids), torch.tensor(labels)\n",
    "\n",
    "X_train, y_train = tokenize_and_align_labels(train_samples)\n",
    "X_valid, y_valid = tokenize_and_align_labels(valid_samples)\n",
    "X_test, y_test   = tokenize_and_align_labels(test_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e6dc28e-32f8-4340-8702-912ceed659d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4125632b-6a87-4b0a-9821-3521724ec908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(MODEL_NAME, num_labels=len(schema))\n",
    "model = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81125f6-de9d-4639-86af-e0498034dede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "EPOCHS = 3\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train_losses, valid_losses = [], []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # Training\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        outputs = model(X).logits\n",
    "        loss = loss_fn(outputs.view(-1, len(schema)), y.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    train_losses.append(total_loss / len(train_loader))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ac8821-4aac-41c2-abf9-c24549dea6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Validation\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in valid_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            outputs = model(X).logits\n",
    "            loss = loss_fn(outputs.view(-1, len(schema)), y.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    valid_losses.append(total_loss / len(valid_loader))\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_losses[-1]:.4f} | Valid Loss: {valid_losses[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97313d1-2d9f-4cbd-9466-e2ad9a0b9dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(valid_losses, label=\"Valid Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe6055b-8b7f-40a0-acf1-313f2fe40216",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Mediapipe)",
   "language": "python",
   "name": "mp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
